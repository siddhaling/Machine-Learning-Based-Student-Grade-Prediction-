# Machine-Learning-Based-Student-Grade-Prediction
 
## Credits: Aastha Jha, Paridhi Garg, Siddhaling Urolaing

Methodologically, analysis of the previous academic performance of students admitted can be used to better predict their future performance using data mining techniques like machine learning. Different data mining techniques can be used to predict students’ grades which will provide valuable information which will help in facilitating students’ retention in the enrolled courses. 
The following diagram shows the step wise description of the grade prediction procedure used in this project. 
![image](https://user-images.githubusercontent.com/33411128/133929668-fbdace3e-7217-49bb-b252-cd3b8dd44794.png)
![image](https://user-images.githubusercontent.com/33411128/133929726-2c7a8580-b4eb-452f-9a33-f827720cf494.png)
![image](https://user-images.githubusercontent.com/33411128/133929744-46036fa2-9942-49ba-b9ed-9e911d6309ce.png)

We displayed 4 bar graphs to depict the accuracy, precision, mean absolute error and true positive rate for all of the 6 techniques so we can compare their performances.
Accuracy: We can see that Random Forest(85%) has the highest accuracy while Naïve Bayesian has the lowest accuracy(40%).

![image](https://user-images.githubusercontent.com/33411128/133929806-6e82cdcb-cba0-4bb6-a17e-15e77f33666d.png)

Mean Absolute Error: We can see that Naïve Bayesian has the highest error(1.05) and Random forest has the lowest error(0.15).
![image](https://user-images.githubusercontent.com/33411128/133929831-f7a71c73-6ab0-44cd-bf32-49568a6b440a.png)

Precision: The highest precision is for Decision trees with 92.31% and lowest precision for SVM with 41.25%.

![image](https://user-images.githubusercontent.com/33411128/133929858-e4b5981d-59cd-4fe9-bdf5-e16fba6cda79.png)

TPR: The highest TPR is for SVM and Random Forest both with the value 1.0 and lowest is Naïve Bayesian with the value 0.42.

![image](https://user-images.githubusercontent.com/33411128/133929876-cfcd15ad-020e-4a30-9b96-894cf3f050f0.png)

Matrices: Confusion matrix for each technique is displayed as a heat map.

Decision tree:

![image](https://user-images.githubusercontent.com/33411128/133929897-01c81f19-87fc-4a33-a25f-173d33a02065.png)

SVM

![image](https://user-images.githubusercontent.com/33411128/133929903-bd0a6b02-38a2-4f94-a357-707d0c8f698c.png)

KNN

![image](https://user-images.githubusercontent.com/33411128/133929912-ec598192-8744-4d6c-8762-1542ea6944a5.png)

Naïve Bayesian

![image](https://user-images.githubusercontent.com/33411128/133929918-52d239f1-72ac-4b0c-b299-36c0c45ff6fa.png)

Random Forest

![image](https://user-images.githubusercontent.com/33411128/133929926-cd635a84-87e1-4d9d-a02b-625e452195f2.png)

Logistic Regression:

![image](https://user-images.githubusercontent.com/33411128/133929937-98c68a6c-61a1-4492-ac1f-251ffc83d058.png)


# Further Projects and Contact
www.researchreader.com

https://medium.com/@dr.siddhaling

Dr. Siddhaling Urolagin,\
PhD, Post-Doc, Machine Learning and Data Science Expert,\
Passionate Researcher, Deep Learning, Machine Learning and applications,\
dr.siddhaling@gmail.com
